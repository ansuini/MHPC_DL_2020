{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST\n",
    "\n",
    "We see FC and convolutional networks (first time) here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNet(nn.Module):\n",
    "    def __init__(self,din=784, dh=30, dout=10):\n",
    "        super(FCNet, self).__init__()\n",
    "        \n",
    "        self.lin1 = nn.Linear(din, dh)\n",
    "        self.lin2 = nn.Linear(dh, dout)\n",
    "    \n",
    "    def forward(self, x):    \n",
    "        x = torch.sigmoid(self.lin1(x)) \n",
    "        x = self.lin2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "?F.nll_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        data = data.view(-1,784)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            data = data.view(-1,784)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "input_size=(1,784,)\n",
    "batch_size=64\n",
    "test_batch_size=1000\n",
    "epochs=10\n",
    "lr=0.01\n",
    "momentum=0.0   \n",
    "seed=1\n",
    "log_interval=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(seed)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "        batch_size=test_batch_size, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FCNet().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1                [-1, 1, 30]          23,550\n",
      "            Linear-2                [-1, 1, 10]             310\n",
      "================================================================\n",
      "Total params: 23,860\n",
      "Trainable params: 23,860\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.00\n",
      "Params size (MB): 0.09\n",
      "Estimated Total Size (MB): 0.09\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model,input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: -0.215291\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: -2.049868\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: -4.413392\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: -6.953173\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: -9.820770\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: -12.895130\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: -15.663527\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: -18.660391\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: -21.821407\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: -24.795416\n",
      "\n",
      "Test set: Average loss: -25.9557, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: -25.997824\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: -29.075413\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: -32.098236\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: -35.382168\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: -38.304573\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: -41.566151\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: -44.632877\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: -47.380630\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: -51.249931\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: -54.656269\n",
      "\n",
      "Test set: Average loss: -55.1103, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: -54.917435\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: -57.629063\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: -61.163940\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: -64.635178\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: -67.627472\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: -70.655304\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: -74.046288\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: -77.576279\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: -80.176308\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: -82.766609\n",
      "\n",
      "Test set: Average loss: -84.2737, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: -84.411179\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: -87.442535\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: -90.685555\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: -92.594566\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: -98.117332\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: -100.182503\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: -103.966682\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: -106.653076\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: -107.122726\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: -113.135735\n",
      "\n",
      "Test set: Average loss: -113.4385, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: -113.416397\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: -116.518585\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: -119.719879\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: -123.041786\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: -125.762833\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: -128.532486\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: -133.913284\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: -136.217987\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: -137.631683\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: -140.562119\n",
      "\n",
      "Test set: Average loss: -142.6034, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: -143.198639\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: -145.721558\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: -150.331635\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: -153.306671\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: -154.505997\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: -158.312836\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: -161.959488\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: -163.786789\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: -167.555847\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: -169.757401\n",
      "\n",
      "Test set: Average loss: -171.7682, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: -171.099533\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: -174.494476\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: -178.696381\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: -182.272675\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: -184.201355\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: -186.190063\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: -189.740234\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: -191.666336\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: -196.703827\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: -199.077423\n",
      "\n",
      "Test set: Average loss: -200.9341, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: -200.817947\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: -202.808090\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: -207.995605\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: -211.217209\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: -217.359375\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: -218.103638\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: -219.823288\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: -225.912491\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: -228.451767\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: -228.684296\n",
      "\n",
      "Test set: Average loss: -230.0987, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: -232.446136\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: -234.615417\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: -238.122375\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: -239.296875\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: -238.448822\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: -246.578003\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: -245.400787\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: -248.434830\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: -256.021118\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: -258.706116\n",
      "\n",
      "Test set: Average loss: -259.2639, Accuracy: 1135/10000 (11%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: -256.968719\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: -262.239288\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: -265.572174\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: -268.398956\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: -269.407806\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: -274.645996\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: -280.859131\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: -280.121521\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: -280.604736\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: -287.788269\n",
      "\n",
      "Test set: Average loss: -288.4304, Accuracy: 1135/10000 (11%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "- The code will run but the network will not train. \n",
    "The problem is that FCNet output is inconsistent with the criterion used.\n",
    "Modify the code in order for the FCNet to work with cross entropy.\n",
    "\n",
    "- Then modify the code accordingly to use ConvNet (Duplicate this notebook if you find useful)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
